train:
  num_epochs: 2
  learning_rate: 5e-5
  gradient_accumulation_steps: 4
  batch_size: 1
  use_task_batch_sampler: false
  warmup_ratio: 0.03
  resume_from_checkpoint: null

  optimizer: paged_adamw_32bit
  max_grad_norm: 0.3
  lr_scheduler_type: cosine
  fp16: true