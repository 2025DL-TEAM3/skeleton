train:
  num_epochs: 5
  learning_rate: 5e-5
  # learning_rate: 1e-4
  gradient_accumulation_steps: 4
  batch_size: 1
  eval_batch_size: 1
  use_task_batch_sampler: false
  warmup_ratio: 0.05
  resume_from_checkpoint: null

  optimizer: paged_adamw_8bit
  max_grad_norm: 1.0
  lr_scheduler_type: linear
  fp16: true

  eval_strategy: "steps"

  use_data_augmentation: true